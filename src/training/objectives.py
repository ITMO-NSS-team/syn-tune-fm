import torch
import torch.nn as nn
import torch.nn.functional as F

class ObjectivesHelper:
    """
    Provides loss functions for model training and fine-tuning.
    """
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.ce_loss = nn.CrossEntropyLoss().to(self.device)

    def compute_cross_entropy(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Standard Cross-Entropy Loss for hard class labels.
        Targets should be integer class indices.
        """
        return self.ce_loss(logits, targets)

    def compute_distillation_loss(self, student_logits: torch.Tensor, teacher_logits: torch.Tensor, temperature: float = 2.0) -> torch.Tensor:
        """
        Knowledge Distillation Loss (KL Divergence).
        Extremely useful when training on synthetic probabilities (soft labels) 
        generated by a larger teacher model or generator.
        """
        # Scale logits by temperature
        soft_targets = F.softmax(teacher_logits / temperature, dim=-1)
        soft_prob = F.log_softmax(student_logits / temperature, dim=-1)
        
        # Calculate KL Divergence
        loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (temperature ** 2)
        return loss